{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PAINTING</th>\n",
       "      <th>mean_fixations_small</th>\n",
       "      <th>mean_saccades_small</th>\n",
       "      <th>mean_diametro_small</th>\n",
       "      <th>min_fixations_small</th>\n",
       "      <th>min_saccades_small</th>\n",
       "      <th>min_diametro_small</th>\n",
       "      <th>max_fixations_small</th>\n",
       "      <th>max_saccades_small</th>\n",
       "      <th>...</th>\n",
       "      <th>max_fixations_full</th>\n",
       "      <th>max_saccades_full</th>\n",
       "      <th>max_diametro_full</th>\n",
       "      <th>std_fixations_full</th>\n",
       "      <th>std_saccades_full</th>\n",
       "      <th>std_diametro_full</th>\n",
       "      <th>num_diff_nonzero_fixations_full</th>\n",
       "      <th>num_diff_nonzero_saccades_full</th>\n",
       "      <th>voto</th>\n",
       "      <th>Binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>533.025280</td>\n",
       "      <td>160.423178</td>\n",
       "      <td>40.676603</td>\n",
       "      <td>236.200000</td>\n",
       "      <td>233.062500</td>\n",
       "      <td>39.534199</td>\n",
       "      <td>1303.125000</td>\n",
       "      <td>414.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>2364</td>\n",
       "      <td>621</td>\n",
       "      <td>56.098621</td>\n",
       "      <td>651.847840</td>\n",
       "      <td>161.042347</td>\n",
       "      <td>13.344737</td>\n",
       "      <td>48</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>959.141261</td>\n",
       "      <td>118.256765</td>\n",
       "      <td>41.718083</td>\n",
       "      <td>543.625000</td>\n",
       "      <td>224.375000</td>\n",
       "      <td>37.589233</td>\n",
       "      <td>1859.500000</td>\n",
       "      <td>399.937500</td>\n",
       "      <td>...</td>\n",
       "      <td>2630</td>\n",
       "      <td>565</td>\n",
       "      <td>80.965363</td>\n",
       "      <td>829.541125</td>\n",
       "      <td>150.093936</td>\n",
       "      <td>12.588451</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>618.470264</td>\n",
       "      <td>157.264000</td>\n",
       "      <td>38.525371</td>\n",
       "      <td>257.750000</td>\n",
       "      <td>232.312500</td>\n",
       "      <td>33.263347</td>\n",
       "      <td>1465.625000</td>\n",
       "      <td>430.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>2211</td>\n",
       "      <td>653</td>\n",
       "      <td>55.277233</td>\n",
       "      <td>685.965108</td>\n",
       "      <td>171.631852</td>\n",
       "      <td>14.020381</td>\n",
       "      <td>51</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>388.826547</td>\n",
       "      <td>172.173720</td>\n",
       "      <td>38.225977</td>\n",
       "      <td>210.250000</td>\n",
       "      <td>233.500000</td>\n",
       "      <td>33.678094</td>\n",
       "      <td>1061.062500</td>\n",
       "      <td>413.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>1686</td>\n",
       "      <td>637</td>\n",
       "      <td>78.819977</td>\n",
       "      <td>481.533646</td>\n",
       "      <td>177.598579</td>\n",
       "      <td>15.688681</td>\n",
       "      <td>52</td>\n",
       "      <td>38</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>416.682170</td>\n",
       "      <td>191.483547</td>\n",
       "      <td>37.850256</td>\n",
       "      <td>193.333333</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>34.204060</td>\n",
       "      <td>1107.562500</td>\n",
       "      <td>523.562500</td>\n",
       "      <td>...</td>\n",
       "      <td>1824</td>\n",
       "      <td>1106</td>\n",
       "      <td>76.008530</td>\n",
       "      <td>534.620834</td>\n",
       "      <td>216.905087</td>\n",
       "      <td>13.772832</td>\n",
       "      <td>53</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>433</td>\n",
       "      <td>21</td>\n",
       "      <td>413.759201</td>\n",
       "      <td>272.099505</td>\n",
       "      <td>43.226224</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>88.125000</td>\n",
       "      <td>14.555465</td>\n",
       "      <td>1131.875000</td>\n",
       "      <td>746.437500</td>\n",
       "      <td>...</td>\n",
       "      <td>2328</td>\n",
       "      <td>1785</td>\n",
       "      <td>94.056313</td>\n",
       "      <td>594.567670</td>\n",
       "      <td>371.586232</td>\n",
       "      <td>19.143375</td>\n",
       "      <td>59</td>\n",
       "      <td>126</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>433</td>\n",
       "      <td>22</td>\n",
       "      <td>713.084441</td>\n",
       "      <td>198.303046</td>\n",
       "      <td>38.480120</td>\n",
       "      <td>196.687500</td>\n",
       "      <td>103.125000</td>\n",
       "      <td>22.076219</td>\n",
       "      <td>1580.875000</td>\n",
       "      <td>638.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>2558</td>\n",
       "      <td>986</td>\n",
       "      <td>64.859375</td>\n",
       "      <td>799.131948</td>\n",
       "      <td>242.222748</td>\n",
       "      <td>10.201133</td>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>433</td>\n",
       "      <td>23</td>\n",
       "      <td>532.374389</td>\n",
       "      <td>233.595994</td>\n",
       "      <td>34.061647</td>\n",
       "      <td>193.375000</td>\n",
       "      <td>96.437500</td>\n",
       "      <td>21.677002</td>\n",
       "      <td>1361.750000</td>\n",
       "      <td>615.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>2622</td>\n",
       "      <td>1197</td>\n",
       "      <td>72.476982</td>\n",
       "      <td>732.873481</td>\n",
       "      <td>269.992910</td>\n",
       "      <td>10.125300</td>\n",
       "      <td>70</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>433</td>\n",
       "      <td>45</td>\n",
       "      <td>562.554638</td>\n",
       "      <td>220.089855</td>\n",
       "      <td>37.764686</td>\n",
       "      <td>128.823529</td>\n",
       "      <td>100.588235</td>\n",
       "      <td>22.597800</td>\n",
       "      <td>1371.058824</td>\n",
       "      <td>596.352941</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>935</td>\n",
       "      <td>84.657867</td>\n",
       "      <td>951.873395</td>\n",
       "      <td>235.670440</td>\n",
       "      <td>16.408542</td>\n",
       "      <td>78</td>\n",
       "      <td>123</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>433</td>\n",
       "      <td>47</td>\n",
       "      <td>541.126109</td>\n",
       "      <td>220.010226</td>\n",
       "      <td>38.175597</td>\n",
       "      <td>180.687500</td>\n",
       "      <td>100.750000</td>\n",
       "      <td>22.556484</td>\n",
       "      <td>1279.750000</td>\n",
       "      <td>635.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>1989</td>\n",
       "      <td>1376</td>\n",
       "      <td>77.669189</td>\n",
       "      <td>611.532328</td>\n",
       "      <td>264.566116</td>\n",
       "      <td>14.669738</td>\n",
       "      <td>67</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3418 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  PAINTING  mean_fixations_small  mean_saccades_small  \\\n",
       "0       1         1            533.025280           160.423178   \n",
       "1       1         2            959.141261           118.256765   \n",
       "2       1         4            618.470264           157.264000   \n",
       "3       1         6            388.826547           172.173720   \n",
       "4       1         7            416.682170           191.483547   \n",
       "...   ...       ...                   ...                  ...   \n",
       "3433  433        21            413.759201           272.099505   \n",
       "3434  433        22            713.084441           198.303046   \n",
       "3435  433        23            532.374389           233.595994   \n",
       "3436  433        45            562.554638           220.089855   \n",
       "3437  433        47            541.126109           220.010226   \n",
       "\n",
       "      mean_diametro_small  min_fixations_small  min_saccades_small  \\\n",
       "0               40.676603           236.200000          233.062500   \n",
       "1               41.718083           543.625000          224.375000   \n",
       "2               38.525371           257.750000          232.312500   \n",
       "3               38.225977           210.250000          233.500000   \n",
       "4               37.850256           193.333333          235.000000   \n",
       "...                   ...                  ...                 ...   \n",
       "3433            43.226224           200.000000           88.125000   \n",
       "3434            38.480120           196.687500          103.125000   \n",
       "3435            34.061647           193.375000           96.437500   \n",
       "3436            37.764686           128.823529          100.588235   \n",
       "3437            38.175597           180.687500          100.750000   \n",
       "\n",
       "      min_diametro_small  max_fixations_small  max_saccades_small  ...  \\\n",
       "0              39.534199          1303.125000          414.062500  ...   \n",
       "1              37.589233          1859.500000          399.937500  ...   \n",
       "2              33.263347          1465.625000          430.750000  ...   \n",
       "3              33.678094          1061.062500          413.250000  ...   \n",
       "4              34.204060          1107.562500          523.562500  ...   \n",
       "...                  ...                  ...                 ...  ...   \n",
       "3433           14.555465          1131.875000          746.437500  ...   \n",
       "3434           22.076219          1580.875000          638.062500  ...   \n",
       "3435           21.677002          1361.750000          615.187500  ...   \n",
       "3436           22.597800          1371.058824          596.352941  ...   \n",
       "3437           22.556484          1279.750000          635.875000  ...   \n",
       "\n",
       "      max_fixations_full  max_saccades_full  max_diametro_full  \\\n",
       "0                   2364                621          56.098621   \n",
       "1                   2630                565          80.965363   \n",
       "2                   2211                653          55.277233   \n",
       "3                   1686                637          78.819977   \n",
       "4                   1824               1106          76.008530   \n",
       "...                  ...                ...                ...   \n",
       "3433                2328               1785          94.056313   \n",
       "3434                2558                986          64.859375   \n",
       "3435                2622               1197          72.476982   \n",
       "3436                3272                935          84.657867   \n",
       "3437                1989               1376          77.669189   \n",
       "\n",
       "      std_fixations_full  std_saccades_full  std_diametro_full  \\\n",
       "0             651.847840         161.042347          13.344737   \n",
       "1             829.541125         150.093936          12.588451   \n",
       "2             685.965108         171.631852          14.020381   \n",
       "3             481.533646         177.598579          15.688681   \n",
       "4             534.620834         216.905087          13.772832   \n",
       "...                  ...                ...                ...   \n",
       "3433          594.567670         371.586232          19.143375   \n",
       "3434          799.131948         242.222748          10.201133   \n",
       "3435          732.873481         269.992910          10.125300   \n",
       "3436          951.873395         235.670440          16.408542   \n",
       "3437          611.532328         264.566116          14.669738   \n",
       "\n",
       "      num_diff_nonzero_fixations_full  num_diff_nonzero_saccades_full  voto  \\\n",
       "0                                  48                              35    25   \n",
       "1                                  40                              28    33   \n",
       "2                                  51                              37    37   \n",
       "3                                  52                              38    18   \n",
       "4                                  53                              33    24   \n",
       "...                               ...                             ...   ...   \n",
       "3433                               59                             126     4   \n",
       "3434                               51                             100    39   \n",
       "3435                               70                             118     2   \n",
       "3436                               78                             123    47   \n",
       "3437                               67                             118     0   \n",
       "\n",
       "      Binary  \n",
       "0          0  \n",
       "1          1  \n",
       "2          1  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "3433       0  \n",
       "3434       1  \n",
       "3435       0  \n",
       "3436       1  \n",
       "3437       0  \n",
       "\n",
       "[3418 rows x 46 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Extracted_Features.csv')\n",
    "data = data.dropna()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "features = ['mean_fixations_small', 'mean_saccades_small', 'mean_diametro_small',\n",
    "            'min_fixations_small', 'min_saccades_small', 'min_diametro_small',\n",
    "            'max_fixations_small', 'max_saccades_small', 'max_diametro_small',\n",
    "            'std_fixations_small', 'std_saccades_small', 'std_diametro_small',\n",
    "            'num_diff_nonzero_fixations_small', 'num_diff_nonzero_saccades_small',\n",
    "            'mean_fixations_medium', 'mean_saccades_medium', 'mean_diametro_medium',\n",
    "            'min_fixations_medium', 'min_saccades_medium', 'min_diametro_medium',\n",
    "            'max_fixations_medium', 'max_saccades_medium', 'max_diametro_medium',\n",
    "            'std_fixations_medium', 'std_saccades_medium', 'std_diametro_medium',\n",
    "            'num_diff_nonzero_fixations_medium', 'num_diff_nonzero_saccades_medium',\n",
    "            'mean_fixations_full', 'mean_saccades_full', 'mean_diametro_full',\n",
    "            'min_fixations_full', 'min_saccades_full', 'min_diametro_full',\n",
    "            'max_fixations_full', 'max_saccades_full', 'max_diametro_full',\n",
    "            'std_fixations_full', 'std_saccades_full', 'std_diametro_full',\n",
    "            'num_diff_nonzero_fixations_full', 'num_diff_nonzero_saccades_full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAINTING: 9\n",
      "Total Samples: 189\n",
      "Percent 0: 19.58%\n",
      "Percent 1: 80.42%\n",
      "Logistic Regression: 0.72\n",
      "SVM: 0.79\n",
      "Random Forest: 0.75\n",
      "Decision Tree: 0.70\n",
      "\n",
      "\n",
      "PAINTING: 10\n",
      "Total Samples: 189\n",
      "Percent 0: 20.11%\n",
      "Percent 1: 79.89%\n",
      "Logistic Regression: 0.70\n",
      "SVM: 0.77\n",
      "Random Forest: 0.72\n",
      "Decision Tree: 0.65\n",
      "\n",
      "\n",
      "PAINTING: 20\n",
      "Total Samples: 188\n",
      "Percent 0: 37.77%\n",
      "Percent 1: 62.23%\n",
      "Logistic Regression: 0.54\n",
      "SVM: 0.70\n",
      "Random Forest: 0.53\n",
      "Decision Tree: 0.44\n",
      "\n",
      "\n",
      "PAINTING: 6\n",
      "Total Samples: 188\n",
      "Percent 0: 37.77%\n",
      "Percent 1: 62.23%\n",
      "Logistic Regression: 0.49\n",
      "SVM: 0.68\n",
      "Random Forest: 0.63\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 11\n",
      "Total Samples: 188\n",
      "Percent 0: 37.77%\n",
      "Percent 1: 62.23%\n",
      "Logistic Regression: 0.60\n",
      "SVM: 0.68\n",
      "Random Forest: 0.54\n",
      "Decision Tree: 0.42\n",
      "\n",
      "\n",
      "PAINTING: 15\n",
      "Total Samples: 190\n",
      "Percent 0: 34.74%\n",
      "Percent 1: 65.26%\n",
      "Logistic Regression: 0.54\n",
      "SVM: 0.68\n",
      "Random Forest: 0.60\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 45\n",
      "Total Samples: 17\n",
      "Percent 0: 58.82%\n",
      "Percent 1: 41.18%\n",
      "Logistic Regression: 0.33\n",
      "SVM: 0.67\n",
      "Random Forest: 0.67\n",
      "Decision Tree: 0.67\n",
      "\n",
      "\n",
      "PAINTING: 2\n",
      "Total Samples: 190\n",
      "Percent 0: 63.68%\n",
      "Percent 1: 36.32%\n",
      "Logistic Regression: 0.53\n",
      "SVM: 0.61\n",
      "Random Forest: 0.60\n",
      "Decision Tree: 0.65\n",
      "\n",
      "\n",
      "PAINTING: 21\n",
      "Total Samples: 189\n",
      "Percent 0: 39.68%\n",
      "Percent 1: 60.32%\n",
      "Logistic Regression: 0.65\n",
      "SVM: 0.61\n",
      "Random Forest: 0.58\n",
      "Decision Tree: 0.60\n",
      "\n",
      "\n",
      "PAINTING: 22\n",
      "Total Samples: 189\n",
      "Percent 0: 36.51%\n",
      "Percent 1: 63.49%\n",
      "Logistic Regression: 0.63\n",
      "SVM: 0.65\n",
      "Random Forest: 0.53\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 13\n",
      "Total Samples: 189\n",
      "Percent 0: 65.08%\n",
      "Percent 1: 34.92%\n",
      "Logistic Regression: 0.54\n",
      "SVM: 0.56\n",
      "Random Forest: 0.51\n",
      "Decision Tree: 0.63\n",
      "\n",
      "\n",
      "PAINTING: 4\n",
      "Total Samples: 189\n",
      "Percent 0: 32.28%\n",
      "Percent 1: 67.72%\n",
      "Logistic Regression: 0.61\n",
      "SVM: 0.54\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.53\n",
      "\n",
      "\n",
      "PAINTING: 14\n",
      "Total Samples: 189\n",
      "Percent 0: 66.67%\n",
      "Percent 1: 33.33%\n",
      "Logistic Regression: 0.51\n",
      "SVM: 0.61\n",
      "Random Forest: 0.61\n",
      "Decision Tree: 0.51\n",
      "\n",
      "\n",
      "PAINTING: 23\n",
      "Total Samples: 189\n",
      "Percent 0: 58.20%\n",
      "Percent 1: 41.80%\n",
      "Logistic Regression: 0.56\n",
      "SVM: 0.56\n",
      "Random Forest: 0.49\n",
      "Decision Tree: 0.61\n",
      "\n",
      "\n",
      "PAINTING: 1\n",
      "Total Samples: 189\n",
      "Percent 0: 45.50%\n",
      "Percent 1: 54.50%\n",
      "Logistic Regression: 0.58\n",
      "SVM: 0.47\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.44\n",
      "\n",
      "\n",
      "PAINTING: 18\n",
      "Total Samples: 190\n",
      "Percent 0: 39.47%\n",
      "Percent 1: 60.53%\n",
      "Logistic Regression: 0.58\n",
      "SVM: 0.58\n",
      "Random Forest: 0.54\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 47\n",
      "Total Samples: 188\n",
      "Percent 0: 56.38%\n",
      "Percent 1: 43.62%\n",
      "Logistic Regression: 0.47\n",
      "SVM: 0.56\n",
      "Random Forest: 0.51\n",
      "Decision Tree: 0.51\n",
      "\n",
      "\n",
      "PAINTING: 7\n",
      "Total Samples: 189\n",
      "Percent 0: 43.92%\n",
      "Percent 1: 56.08%\n",
      "Logistic Regression: 0.54\n",
      "SVM: 0.53\n",
      "Random Forest: 0.54\n",
      "Decision Tree: 0.51\n",
      "\n",
      "\n",
      "PAINTING: 16\n",
      "Total Samples: 189\n",
      "Percent 0: 59.26%\n",
      "Percent 1: 40.74%\n",
      "Logistic Regression: 0.47\n",
      "SVM: 0.53\n",
      "Random Forest: 0.51\n",
      "Decision Tree: 0.53\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[features]\n",
    "y = data['Binary']\n",
    "\n",
    "# Dizionario per memorizzare i risultati\n",
    "results = {}\n",
    "\n",
    "for painting in data['PAINTING'].unique():\n",
    "    subset = data[data['PAINTING'] == painting]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subset[features], subset['Binary'], test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Calcolo delle percentuali di 0 e 1 nella colonna 'Binary'\n",
    "    count_binary = subset['Binary'].value_counts(normalize=True) * 100\n",
    "    percent_0 = count_binary.get(0, 0)\n",
    "    percent_1 = count_binary.get(1, 0)\n",
    "    \n",
    "    # Creazione dei modelli\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "        'SVM': SVC(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier()\n",
    "    }\n",
    "    \n",
    "    # Dizionario per memorizzare l'accuratezza per ogni modello\n",
    "    accuracies = {}\n",
    "    \n",
    "    # Training e test per ogni modello\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracies[model_name] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Memorizzazione dei risultati\n",
    "    results[painting] = {\n",
    "        'Accuracy': accuracies,\n",
    "        'Percent 0': percent_0,\n",
    "        'Percent 1': percent_1,\n",
    "        'Total Samples': len(subset)\n",
    "    }\n",
    "\n",
    "# Step 3: Ordinamento dei risultati\n",
    "sorted_results = {k: v for k, v in sorted(results.items(), key=lambda item: max(item[1]['Accuracy'].values()), reverse=True)}\n",
    "\n",
    "# Stampa dei risultati in formato report\n",
    "for painting, info in sorted_results.items():\n",
    "    print(f\"PAINTING: {painting}\")\n",
    "    print(f\"Total Samples: {info['Total Samples']}\")\n",
    "    print(f\"Percent 0: {info['Percent 0']:.2f}%\")\n",
    "    print(f\"Percent 1: {info['Percent 1']:.2f}%\")\n",
    "    for model, accuracy in info['Accuracy'].items():\n",
    "        print(f\"{model}: {accuracy:.2f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training e Testing con cross-validation di secondo livello\n",
    "\n",
    "Bilancio il dataset\n",
    "Train-Validation-Test Split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "PAINTING: 14\n",
      "Total Samples: 126\n",
      "Logistic Regression: 0.53\n",
      "SVM: 0.54\n",
      "Random Forest: 0.52\n",
      "Decision Tree: 0.63\n",
      "\n",
      "\n",
      "PAINTING: 13\n",
      "Total Samples: 132\n",
      "Logistic Regression: 0.44\n",
      "SVM: 0.49\n",
      "Random Forest: 0.55\n",
      "Decision Tree: 0.63\n",
      "\n",
      "\n",
      "PAINTING: 15\n",
      "Total Samples: 132\n",
      "Logistic Regression: 0.51\n",
      "SVM: 0.63\n",
      "Random Forest: 0.59\n",
      "Decision Tree: 0.49\n",
      "\n",
      "\n",
      "PAINTING: 9\n",
      "Total Samples: 74\n",
      "Logistic Regression: 0.46\n",
      "SVM: 0.47\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.63\n",
      "\n",
      "\n",
      "PAINTING: 47\n",
      "Total Samples: 164\n",
      "Logistic Regression: 0.48\n",
      "SVM: 0.56\n",
      "Random Forest: 0.54\n",
      "Decision Tree: 0.61\n",
      "\n",
      "\n",
      "PAINTING: 4\n",
      "Total Samples: 122\n",
      "Logistic Regression: 0.51\n",
      "SVM: 0.49\n",
      "Random Forest: 0.59\n",
      "Decision Tree: 0.55\n",
      "\n",
      "\n",
      "PAINTING: 16\n",
      "Total Samples: 154\n",
      "Logistic Regression: 0.46\n",
      "SVM: 0.49\n",
      "Random Forest: 0.53\n",
      "Decision Tree: 0.59\n",
      "\n",
      "\n",
      "PAINTING: 21\n",
      "Total Samples: 150\n",
      "Logistic Regression: 0.56\n",
      "SVM: 0.47\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.58\n",
      "\n",
      "\n",
      "PAINTING: 20\n",
      "Total Samples: 142\n",
      "Logistic Regression: 0.55\n",
      "SVM: 0.51\n",
      "Random Forest: 0.57\n",
      "Decision Tree: 0.57\n",
      "\n",
      "\n",
      "PAINTING: 45\n",
      "Total Samples: 14\n",
      "Logistic Regression: 0.07\n",
      "SVM: 0.57\n",
      "Random Forest: 0.33\n",
      "Decision Tree: 0.33\n",
      "\n",
      "\n",
      "PAINTING: 23\n",
      "Total Samples: 158\n",
      "Logistic Regression: 0.42\n",
      "SVM: 0.44\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.52\n",
      "\n",
      "\n",
      "PAINTING: 18\n",
      "Total Samples: 150\n",
      "Logistic Regression: 0.53\n",
      "SVM: 0.46\n",
      "Random Forest: 0.47\n",
      "Decision Tree: 0.56\n",
      "\n",
      "\n",
      "PAINTING: 6\n",
      "Total Samples: 142\n",
      "Logistic Regression: 0.49\n",
      "SVM: 0.54\n",
      "Random Forest: 0.50\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 10\n",
      "Total Samples: 76\n",
      "Logistic Regression: 0.49\n",
      "SVM: 0.47\n",
      "Random Forest: 0.45\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 1\n",
      "Total Samples: 172\n",
      "Logistic Regression: 0.52\n",
      "SVM: 0.53\n",
      "Random Forest: 0.50\n",
      "Decision Tree: 0.48\n",
      "\n",
      "\n",
      "PAINTING: 22\n",
      "Total Samples: 138\n",
      "Logistic Regression: 0.44\n",
      "SVM: 0.44\n",
      "Random Forest: 0.52\n",
      "Decision Tree: 0.49\n",
      "\n",
      "\n",
      "PAINTING: 11\n",
      "Total Samples: 142\n",
      "Logistic Regression: 0.51\n",
      "SVM: 0.44\n",
      "Random Forest: 0.47\n",
      "Decision Tree: 0.49\n",
      "\n",
      "\n",
      "PAINTING: 2\n",
      "Total Samples: 138\n",
      "Logistic Regression: 0.51\n",
      "SVM: 0.39\n",
      "Random Forest: 0.48\n",
      "Decision Tree: 0.48\n",
      "\n",
      "\n",
      "PAINTING: 7\n",
      "Total Samples: 166\n",
      "Logistic Regression: 0.49\n",
      "SVM: 0.45\n",
      "Random Forest: 0.43\n",
      "Decision Tree: 0.40\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = data[features]\n",
    "y = data['Binary']\n",
    "\n",
    "# Dizionario per memorizzare i risultati\n",
    "results = {}\n",
    "\n",
    "for painting in data['PAINTING'].unique():\n",
    "    subset = data[data['PAINTING'] == painting]\n",
    "    \n",
    "    # Bilanciamento delle classi\n",
    "    min_class_size = subset['Binary'].value_counts().min()\n",
    "    balanced_subset = subset.groupby('Binary').sample(n=min_class_size, random_state=42)\n",
    "    \n",
    "    # Suddivisione del dataset in train e test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(balanced_subset[features], balanced_subset['Binary'], test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Preparazione per cross-validation di secondo livello\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_results = {model_name: [] for model_name in ['Logistic Regression', 'SVM', 'Random Forest', 'Decision Tree']}\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_cv, X_val_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        # Creazione dei modelli\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "            'SVM': SVC(),\n",
    "            'Random Forest': RandomForestClassifier(),\n",
    "            'Decision Tree': DecisionTreeClassifier()\n",
    "        }\n",
    "        \n",
    "        # Training e validazione per ogni modello e fold\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train_cv, y_train_cv)\n",
    "            y_pred_cv = model.predict(X_val_cv)\n",
    "            cv_results[model_name].append(accuracy_score(y_val_cv, y_pred_cv))\n",
    "    \n",
    "    # Calcolo della media delle performance dei modelli sulla cross-validation\n",
    "    average_cv_accuracy = {model: np.mean(scores) for model, scores in cv_results.items()}\n",
    "    \n",
    "    # Memorizzazione dei risultati\n",
    "    results[painting] = {\n",
    "        'CV Average Accuracy': average_cv_accuracy,\n",
    "        'Total Samples': len(balanced_subset)\n",
    "    }\n",
    "    print(painting)\n",
    "\n",
    "# Ordinamento dei risultati basato sulla migliore accuracy media di CV\n",
    "sorted_results = {k: v for k, v in sorted(results.items(), key=lambda item: max(item[1]['CV Average Accuracy'].values()), reverse=True)}\n",
    "\n",
    "# Stampa dei risultati in formato report\n",
    "for painting, info in sorted_results.items():\n",
    "    print(f\"PAINTING: {painting}\")\n",
    "    print(f\"Total Samples: {info['Total Samples']}\")\n",
    "    for model, avg_accuracy in info['CV Average Accuracy'].items():\n",
    "        print(f\"{model}: {avg_accuracy:.2f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAINTING: 10\n",
      "Total Samples: 76\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.43\n",
      "SVM: 0.48\n",
      "Random Forest: 0.74\n",
      "Decision Tree: 0.52\n",
      "\n",
      "\n",
      "PAINTING: 15\n",
      "Total Samples: 132\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.65\n",
      "SVM: 0.60\n",
      "Random Forest: 0.55\n",
      "Decision Tree: 0.68\n",
      "\n",
      "\n",
      "PAINTING: 13\n",
      "Total Samples: 132\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.45\n",
      "SVM: 0.42\n",
      "Random Forest: 0.62\n",
      "Decision Tree: 0.65\n",
      "\n",
      "\n",
      "PAINTING: 22\n",
      "Total Samples: 138\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.40\n",
      "SVM: 0.48\n",
      "Random Forest: 0.52\n",
      "Decision Tree: 0.64\n",
      "\n",
      "\n",
      "PAINTING: 47\n",
      "Total Samples: 164\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.40\n",
      "SVM: 0.62\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.64\n",
      "\n",
      "\n",
      "PAINTING: 4\n",
      "Total Samples: 122\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.38\n",
      "SVM: 0.51\n",
      "Random Forest: 0.62\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 6\n",
      "Total Samples: 142\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.53\n",
      "SVM: 0.58\n",
      "Random Forest: 0.60\n",
      "Decision Tree: 0.58\n",
      "\n",
      "\n",
      "PAINTING: 18\n",
      "Total Samples: 150\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.47\n",
      "SVM: 0.40\n",
      "Random Forest: 0.60\n",
      "Decision Tree: 0.58\n",
      "\n",
      "\n",
      "PAINTING: 45\n",
      "Total Samples: 14\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.20\n",
      "SVM: 0.40\n",
      "Random Forest: 0.40\n",
      "Decision Tree: 0.60\n",
      "\n",
      "\n",
      "PAINTING: 16\n",
      "Total Samples: 154\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.43\n",
      "SVM: 0.47\n",
      "Random Forest: 0.51\n",
      "Decision Tree: 0.60\n",
      "\n",
      "\n",
      "PAINTING: 2\n",
      "Total Samples: 138\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.50\n",
      "SVM: 0.48\n",
      "Random Forest: 0.60\n",
      "Decision Tree: 0.43\n",
      "\n",
      "\n",
      "PAINTING: 20\n",
      "Total Samples: 142\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.53\n",
      "SVM: 0.51\n",
      "Random Forest: 0.49\n",
      "Decision Tree: 0.58\n",
      "\n",
      "\n",
      "PAINTING: 14\n",
      "Total Samples: 126\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.58\n",
      "SVM: 0.45\n",
      "Random Forest: 0.55\n",
      "Decision Tree: 0.53\n",
      "\n",
      "\n",
      "PAINTING: 1\n",
      "Total Samples: 172\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.58\n",
      "SVM: 0.58\n",
      "Random Forest: 0.44\n",
      "Decision Tree: 0.40\n",
      "\n",
      "\n",
      "PAINTING: 7\n",
      "Total Samples: 166\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.44\n",
      "SVM: 0.56\n",
      "Random Forest: 0.44\n",
      "Decision Tree: 0.52\n",
      "\n",
      "\n",
      "PAINTING: 11\n",
      "Total Samples: 142\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.56\n",
      "SVM: 0.44\n",
      "Random Forest: 0.40\n",
      "Decision Tree: 0.49\n",
      "\n",
      "\n",
      "PAINTING: 21\n",
      "Total Samples: 150\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.53\n",
      "SVM: 0.38\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.44\n",
      "\n",
      "\n",
      "PAINTING: 23\n",
      "Total Samples: 158\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.50\n",
      "SVM: 0.42\n",
      "Random Forest: 0.46\n",
      "Decision Tree: 0.48\n",
      "\n",
      "\n",
      "PAINTING: 9\n",
      "Total Samples: 74\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.35\n",
      "SVM: 0.35\n",
      "Random Forest: 0.22\n",
      "Decision Tree: 0.48\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[features]\n",
    "y = data['Binary']\n",
    "\n",
    "# Dizionario per memorizzare i risultati\n",
    "results = {}\n",
    "\n",
    "for painting in data['PAINTING'].unique():\n",
    "    subset = data[data['PAINTING'] == painting]\n",
    "\n",
    "    # Bilanciamento delle classi\n",
    "    min_class_size = subset['Binary'].value_counts().min()\n",
    "    balanced_subset = subset.groupby('Binary').sample(n=min_class_size, random_state=42)\n",
    "\n",
    "    subset = balanced_subset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subset[features], subset['Binary'], test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Calcolo delle percentuali di 0 e 1 nella colonna 'Binary'\n",
    "    count_binary = subset['Binary'].value_counts(normalize=True) * 100\n",
    "    percent_0 = count_binary.get(0, 0)\n",
    "    percent_1 = count_binary.get(1, 0)\n",
    "    \n",
    "    # Creazione dei modelli\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "        'SVM': SVC(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier()\n",
    "    }\n",
    "    \n",
    "    # Dizionario per memorizzare l'accuratezza per ogni modello\n",
    "    accuracies = {}\n",
    "    \n",
    "    # Training e test per ogni modello\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracies[model_name] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Memorizzazione dei risultati\n",
    "    results[painting] = {\n",
    "        'Accuracy': accuracies,\n",
    "        'Percent 0': percent_0,\n",
    "        'Percent 1': percent_1,\n",
    "        'Total Samples': len(subset)\n",
    "    }\n",
    "\n",
    "# Step 3: Ordinamento dei risultati\n",
    "sorted_results = {k: v for k, v in sorted(results.items(), key=lambda item: max(item[1]['Accuracy'].values()), reverse=True)}\n",
    "\n",
    "# Stampa dei risultati in formato report\n",
    "for painting, info in sorted_results.items():\n",
    "    print(f\"PAINTING: {painting}\")\n",
    "    print(f\"Total Samples: {info['Total Samples']}\")\n",
    "    print(f\"Percent 0: {info['Percent 0']:.2f}%\")\n",
    "    print(f\"Percent 1: {info['Percent 1']:.2f}%\")\n",
    "    for model, accuracy in info['Accuracy'].items():\n",
    "        print(f\"{model}: {accuracy:.2f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: {'Accuracy': {'Logistic Regression': 0.43478260869565216, 'SVM': 0.4782608695652174, 'Random Forest': 0.7391304347826086, 'Decision Tree': 0.5217391304347826}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 76}, 15: {'Accuracy': {'Logistic Regression': 0.65, 'SVM': 0.6, 'Random Forest': 0.55, 'Decision Tree': 0.675}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 132}, 13: {'Accuracy': {'Logistic Regression': 0.45, 'SVM': 0.425, 'Random Forest': 0.625, 'Decision Tree': 0.65}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 132}, 22: {'Accuracy': {'Logistic Regression': 0.40476190476190477, 'SVM': 0.47619047619047616, 'Random Forest': 0.5238095238095238, 'Decision Tree': 0.6428571428571429}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 138}, 47: {'Accuracy': {'Logistic Regression': 0.4, 'SVM': 0.62, 'Random Forest': 0.56, 'Decision Tree': 0.64}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 164}, 4: {'Accuracy': {'Logistic Regression': 0.3783783783783784, 'SVM': 0.5135135135135135, 'Random Forest': 0.6216216216216216, 'Decision Tree': 0.5405405405405406}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 122}, 6: {'Accuracy': {'Logistic Regression': 0.5348837209302325, 'SVM': 0.5813953488372093, 'Random Forest': 0.6046511627906976, 'Decision Tree': 0.5813953488372093}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 142}, 18: {'Accuracy': {'Logistic Regression': 0.4666666666666667, 'SVM': 0.4, 'Random Forest': 0.6, 'Decision Tree': 0.5777777777777777}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 150}, 45: {'Accuracy': {'Logistic Regression': 0.2, 'SVM': 0.4, 'Random Forest': 0.4, 'Decision Tree': 0.6}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 14}, 16: {'Accuracy': {'Logistic Regression': 0.425531914893617, 'SVM': 0.46808510638297873, 'Random Forest': 0.5106382978723404, 'Decision Tree': 0.5957446808510638}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 154}, 2: {'Accuracy': {'Logistic Regression': 0.5, 'SVM': 0.47619047619047616, 'Random Forest': 0.5952380952380952, 'Decision Tree': 0.42857142857142855}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 138}, 20: {'Accuracy': {'Logistic Regression': 0.5348837209302325, 'SVM': 0.5116279069767442, 'Random Forest': 0.4883720930232558, 'Decision Tree': 0.5813953488372093}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 142}, 14: {'Accuracy': {'Logistic Regression': 0.5789473684210527, 'SVM': 0.4473684210526316, 'Random Forest': 0.5526315789473685, 'Decision Tree': 0.5263157894736842}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 126}, 1: {'Accuracy': {'Logistic Regression': 0.5769230769230769, 'SVM': 0.5769230769230769, 'Random Forest': 0.4423076923076923, 'Decision Tree': 0.40384615384615385}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 172}, 7: {'Accuracy': {'Logistic Regression': 0.44, 'SVM': 0.56, 'Random Forest': 0.44, 'Decision Tree': 0.52}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 166}, 11: {'Accuracy': {'Logistic Regression': 0.5581395348837209, 'SVM': 0.4418604651162791, 'Random Forest': 0.3953488372093023, 'Decision Tree': 0.4883720930232558}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 142}, 21: {'Accuracy': {'Logistic Regression': 0.5333333333333333, 'SVM': 0.37777777777777777, 'Random Forest': 0.5555555555555556, 'Decision Tree': 0.4444444444444444}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 150}, 23: {'Accuracy': {'Logistic Regression': 0.5, 'SVM': 0.4166666666666667, 'Random Forest': 0.4583333333333333, 'Decision Tree': 0.4791666666666667}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 158}, 9: {'Accuracy': {'Logistic Regression': 0.34782608695652173, 'SVM': 0.34782608695652173, 'Random Forest': 0.21739130434782608, 'Decision Tree': 0.4782608695652174}, 'Percent 0': 50.0, 'Percent 1': 50.0, 'Total Samples': 74}}\n"
     ]
    }
   ],
   "source": [
    "print(sorted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 0.7391304347826086],\n",
       " [15, 0.675],\n",
       " [13, 0.65],\n",
       " [22, 0.6428571428571429],\n",
       " [47, 0.64],\n",
       " [4, 0.6216216216216216],\n",
       " [6, 0.6046511627906976],\n",
       " [18, 0.6],\n",
       " [45, 0.6],\n",
       " [16, 0.5957446808510638],\n",
       " [2, 0.5952380952380952],\n",
       " [20, 0.5813953488372093],\n",
       " [14, 0.5789473684210527],\n",
       " [1, 0.5769230769230769],\n",
       " [7, 0.56],\n",
       " [11, 0.5581395348837209],\n",
       " [21, 0.5555555555555556],\n",
       " [23, 0.5],\n",
       " [9, 0.4782608695652174]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista per contenere le coppie [id, max_accuracy]\n",
    "max_accuracy_list = []\n",
    "\n",
    "# Estrazione dell'accuracy più alta per ogni ID\n",
    "for id, values in sorted_results.items():\n",
    "    max_accuracy = max(values['Accuracy'].values())  # Trova il massimo tra le accuracies\n",
    "    max_accuracy_list.append([id, max_accuracy])  # Aggiunge la coppia [id, max_accuracy] alla lista\n",
    "\n",
    "max_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 26.566137566137566), (2, 22.48421052631579), (4, 29.61904761904762), (6, 28.45744680851064), (7, 26.476190476190474), (9, 33.82539682539682), (10, 33.370370370370374), (11, 27.99468085106383), (13, 20.095238095238095), (14, 19.693121693121693), (15, 29.442105263157895), (16, 21.232804232804234), (18, 27.07894736842105), (20, 28.4468085106383), (21, 27.375661375661377), (22, 28.59259259259259), (23, 22.798941798941797), (45, 23.352941176470587), (47, 22.585106382978722)]\n"
     ]
    }
   ],
   "source": [
    "voto_medio_per_painting = data.groupby('PAINTING')['voto'].mean().reset_index()\n",
    "\n",
    "# Rinominiamo le colonne per chiarezza\n",
    "voto_medio_per_painting.columns = ['PAINTING', 'Voto Medio']\n",
    "\n",
    "# Convertiamo il DataFrame in una lista di tuple\n",
    "lista_voti_medio = list(voto_medio_per_painting.itertuples(index=False, name=None))\n",
    "\n",
    "# Mostra la lista\n",
    "print(lista_voti_medio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertiamo le liste in DataFrame\n",
    "df_accuracy = pd.DataFrame(max_accuracy_list, columns=['id', 'accuracy'])\n",
    "df_voti = pd.DataFrame(lista_voti_medio, columns=['id', 'voto'])\n",
    "\n",
    "# Uniamo i DataFrame sui valori di 'id'\n",
    "df_merged = pd.merge(df_accuracy, df_voti, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creazione dello scatter plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df_merged['voto'], df_merged['accuracy'], color='blue', alpha=0.5)\n",
    "\n",
    "# Aggiungiamo titoli agli assi e al grafico\n",
    "plt.title('Scatter Plot di Accuracy vs Voto')\n",
    "plt.xlabel('Voto Medio')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['min_saccades_medium',\n",
    " 'min_saccades_small',\n",
    " 'min_saccades_full',\n",
    " 'num_diff_nonzero_saccades_medium',\n",
    " 'num_diff_nonzero_saccades_small',\n",
    " 'num_diff_nonzero_saccades_full',\n",
    " 'min_diametro_full',\n",
    " 'num_diff_nonzero_fixations_full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAINTING: 45\n",
      "Total Samples: 14\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.00\n",
      "SVM: 0.43\n",
      "Random Forest: 0.71\n",
      "Decision Tree: 0.43\n",
      "\n",
      "\n",
      "PAINTING: 6\n",
      "Total Samples: 142\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.55\n",
      "SVM: 0.46\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.66\n",
      "\n",
      "\n",
      "PAINTING: 14\n",
      "Total Samples: 126\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.54\n",
      "SVM: 0.59\n",
      "Random Forest: 0.65\n",
      "Decision Tree: 0.56\n",
      "\n",
      "\n",
      "PAINTING: 10\n",
      "Total Samples: 76\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.53\n",
      "SVM: 0.58\n",
      "Random Forest: 0.58\n",
      "Decision Tree: 0.63\n",
      "\n",
      "\n",
      "PAINTING: 11\n",
      "Total Samples: 142\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.61\n",
      "SVM: 0.45\n",
      "Random Forest: 0.58\n",
      "Decision Tree: 0.44\n",
      "\n",
      "\n",
      "PAINTING: 20\n",
      "Total Samples: 142\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.54\n",
      "SVM: 0.59\n",
      "Random Forest: 0.54\n",
      "Decision Tree: 0.48\n",
      "\n",
      "\n",
      "PAINTING: 47\n",
      "Total Samples: 164\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.54\n",
      "SVM: 0.46\n",
      "Random Forest: 0.56\n",
      "Decision Tree: 0.59\n",
      "\n",
      "\n",
      "PAINTING: 15\n",
      "Total Samples: 132\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.47\n",
      "SVM: 0.50\n",
      "Random Forest: 0.55\n",
      "Decision Tree: 0.58\n",
      "\n",
      "\n",
      "PAINTING: 18\n",
      "Total Samples: 150\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.49\n",
      "SVM: 0.45\n",
      "Random Forest: 0.51\n",
      "Decision Tree: 0.55\n",
      "\n",
      "\n",
      "PAINTING: 1\n",
      "Total Samples: 172\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.50\n",
      "SVM: 0.47\n",
      "Random Forest: 0.55\n",
      "Decision Tree: 0.52\n",
      "\n",
      "\n",
      "PAINTING: 4\n",
      "Total Samples: 122\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.49\n",
      "SVM: 0.48\n",
      "Random Forest: 0.48\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 21\n",
      "Total Samples: 150\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.49\n",
      "SVM: 0.51\n",
      "Random Forest: 0.47\n",
      "Decision Tree: 0.53\n",
      "\n",
      "\n",
      "PAINTING: 16\n",
      "Total Samples: 154\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.51\n",
      "SVM: 0.45\n",
      "Random Forest: 0.53\n",
      "Decision Tree: 0.48\n",
      "\n",
      "\n",
      "PAINTING: 23\n",
      "Total Samples: 158\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.53\n",
      "SVM: 0.43\n",
      "Random Forest: 0.52\n",
      "Decision Tree: 0.46\n",
      "\n",
      "\n",
      "PAINTING: 7\n",
      "Total Samples: 166\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.49\n",
      "SVM: 0.46\n",
      "Random Forest: 0.53\n",
      "Decision Tree: 0.48\n",
      "\n",
      "\n",
      "PAINTING: 9\n",
      "Total Samples: 74\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.41\n",
      "SVM: 0.51\n",
      "Random Forest: 0.46\n",
      "Decision Tree: 0.49\n",
      "\n",
      "\n",
      "PAINTING: 2\n",
      "Total Samples: 138\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.51\n",
      "SVM: 0.42\n",
      "Random Forest: 0.43\n",
      "Decision Tree: 0.45\n",
      "\n",
      "\n",
      "PAINTING: 22\n",
      "Total Samples: 138\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.42\n",
      "SVM: 0.45\n",
      "Random Forest: 0.51\n",
      "Decision Tree: 0.48\n",
      "\n",
      "\n",
      "PAINTING: 13\n",
      "Total Samples: 132\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.48\n",
      "SVM: 0.48\n",
      "Random Forest: 0.48\n",
      "Decision Tree: 0.44\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[features]\n",
    "y = data['Binary']\n",
    "\n",
    "# Dizionario per memorizzare i risultati\n",
    "results = {}\n",
    "\n",
    "for painting in data['PAINTING'].unique():\n",
    "    subset = data[data['PAINTING'] == painting]\n",
    "\n",
    "    # Bilanciamento delle classi\n",
    "    min_class_size = subset['Binary'].value_counts().min()\n",
    "    balanced_subset = subset.groupby('Binary').sample(n=min_class_size, random_state=42)\n",
    "\n",
    "    subset = balanced_subset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subset[features], subset['Binary'], test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Calcolo delle percentuali di 0 e 1 nella colonna 'Binary'\n",
    "    count_binary = subset['Binary'].value_counts(normalize=True) * 100\n",
    "    percent_0 = count_binary.get(0, 0)\n",
    "    percent_1 = count_binary.get(1, 0)\n",
    "    \n",
    "    # Creazione dei modelli\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "        'SVM': SVC(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier()\n",
    "    }\n",
    "    \n",
    "    # Dizionario per memorizzare l'accuratezza per ogni modello\n",
    "    accuracies = {}\n",
    "    \n",
    "    # Training e test per ogni modello\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracies[model_name] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Memorizzazione dei risultati\n",
    "    results[painting] = {\n",
    "        'Accuracy': accuracies,\n",
    "        'Percent 0': percent_0,\n",
    "        'Percent 1': percent_1,\n",
    "        'Total Samples': len(subset)\n",
    "    }\n",
    "\n",
    "# Step 3: Ordinamento dei risultati\n",
    "sorted_results = {k: v for k, v in sorted(results.items(), key=lambda item: max(item[1]['Accuracy'].values()), reverse=True)}\n",
    "\n",
    "# Stampa dei risultati in formato report\n",
    "for painting, info in sorted_results.items():\n",
    "    print(f\"PAINTING: {painting}\")\n",
    "    print(f\"Total Samples: {info['Total Samples']}\")\n",
    "    print(f\"Percent 0: {info['Percent 0']:.2f}%\")\n",
    "    print(f\"Percent 1: {info['Percent 1']:.2f}%\")\n",
    "    for model, accuracy in info['Accuracy'].items():\n",
    "        print(f\"{model}: {accuracy:.2f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[45, 0.7142857142857143],\n",
       " [6, 0.6619718309859155],\n",
       " [14, 0.6507936507936508],\n",
       " [10, 0.631578947368421],\n",
       " [11, 0.6056338028169014],\n",
       " [20, 0.5915492957746479],\n",
       " [47, 0.5853658536585366],\n",
       " [15, 0.5757575757575758],\n",
       " [18, 0.5466666666666666],\n",
       " [1, 0.5465116279069767],\n",
       " [4, 0.5409836065573771],\n",
       " [21, 0.5333333333333333],\n",
       " [16, 0.5324675324675324],\n",
       " [23, 0.5316455696202531],\n",
       " [7, 0.5301204819277109],\n",
       " [9, 0.5135135135135135],\n",
       " [2, 0.5072463768115942],\n",
       " [22, 0.5072463768115942],\n",
       " [13, 0.48484848484848486]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista per contenere le coppie [id, max_accuracy]\n",
    "max_accuracy_list = []\n",
    "\n",
    "# Estrazione dell'accuracy più alta per ogni ID\n",
    "for id, values in sorted_results.items():\n",
    "    max_accuracy = max(values['Accuracy'].values())  # Trova il massimo tra le accuracies\n",
    "    max_accuracy_list.append([id, max_accuracy])  # Aggiunge la coppia [id, max_accuracy] alla lista\n",
    "\n",
    "max_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 26.566137566137566), (2, 22.48421052631579), (4, 29.61904761904762), (6, 28.45744680851064), (7, 26.476190476190474), (9, 33.82539682539682), (10, 33.370370370370374), (11, 27.99468085106383), (13, 20.095238095238095), (14, 19.693121693121693), (15, 29.442105263157895), (16, 21.232804232804234), (18, 27.07894736842105), (20, 28.4468085106383), (21, 27.375661375661377), (22, 28.59259259259259), (23, 22.798941798941797), (45, 23.352941176470587), (47, 22.585106382978722)]\n"
     ]
    }
   ],
   "source": [
    "voto_medio_per_painting = data.groupby('PAINTING')['voto'].mean().reset_index()\n",
    "\n",
    "# Rinominiamo le colonne per chiarezza\n",
    "voto_medio_per_painting.columns = ['PAINTING', 'Voto Medio']\n",
    "\n",
    "# Convertiamo il DataFrame in una lista di tuple\n",
    "lista_voti_medio = list(voto_medio_per_painting.itertuples(index=False, name=None))\n",
    "\n",
    "# Mostra la lista\n",
    "print(lista_voti_medio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertiamo le liste in DataFrame\n",
    "df_accuracy = pd.DataFrame(max_accuracy_list, columns=['id', 'accuracy'])\n",
    "df_voti = pd.DataFrame(lista_voti_medio, columns=['id', 'voto'])\n",
    "\n",
    "# Uniamo i DataFrame sui valori di 'id'\n",
    "df_merged = pd.merge(df_accuracy, df_voti, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creazione dello scatter plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df_merged['voto'], df_merged['accuracy'], color='blue', alpha=0.5)\n",
    "\n",
    "# Aggiungiamo titoli agli assi e al grafico\n",
    "plt.title('Scatter Plot di Accuracy vs Voto')\n",
    "plt.xlabel('Voto Medio')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PAINTING</th>\n",
       "      <th>mean_fixations_small</th>\n",
       "      <th>mean_saccades_small</th>\n",
       "      <th>mean_diametro_small</th>\n",
       "      <th>min_fixations_small</th>\n",
       "      <th>min_saccades_small</th>\n",
       "      <th>min_diametro_small</th>\n",
       "      <th>max_fixations_small</th>\n",
       "      <th>max_saccades_small</th>\n",
       "      <th>...</th>\n",
       "      <th>max_fixations_full</th>\n",
       "      <th>max_saccades_full</th>\n",
       "      <th>max_diametro_full</th>\n",
       "      <th>std_fixations_full</th>\n",
       "      <th>std_saccades_full</th>\n",
       "      <th>std_diametro_full</th>\n",
       "      <th>num_diff_nonzero_fixations_full</th>\n",
       "      <th>num_diff_nonzero_saccades_full</th>\n",
       "      <th>voto</th>\n",
       "      <th>Binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>16163.011484</td>\n",
       "      <td>7.248903</td>\n",
       "      <td>1.669092</td>\n",
       "      <td>15893.937500</td>\n",
       "      <td>332.600000</td>\n",
       "      <td>23.893966</td>\n",
       "      <td>17257.235294</td>\n",
       "      <td>100.117647</td>\n",
       "      <td>...</td>\n",
       "      <td>26047</td>\n",
       "      <td>379</td>\n",
       "      <td>48.796249</td>\n",
       "      <td>7960.700631</td>\n",
       "      <td>52.836893</td>\n",
       "      <td>7.288566</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>2269.519269</td>\n",
       "      <td>118.094715</td>\n",
       "      <td>11.386530</td>\n",
       "      <td>1671.125000</td>\n",
       "      <td>242.642857</td>\n",
       "      <td>26.685896</td>\n",
       "      <td>3444.000000</td>\n",
       "      <td>464.562500</td>\n",
       "      <td>...</td>\n",
       "      <td>7972</td>\n",
       "      <td>1146</td>\n",
       "      <td>45.317871</td>\n",
       "      <td>2647.314151</td>\n",
       "      <td>212.889846</td>\n",
       "      <td>17.403155</td>\n",
       "      <td>45</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>100</td>\n",
       "      <td>45</td>\n",
       "      <td>1205.692659</td>\n",
       "      <td>112.501913</td>\n",
       "      <td>53.129677</td>\n",
       "      <td>371.750000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>49.204310</td>\n",
       "      <td>2047.812500</td>\n",
       "      <td>321.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>5536</td>\n",
       "      <td>537</td>\n",
       "      <td>59.457198</td>\n",
       "      <td>1558.796177</td>\n",
       "      <td>123.704900</td>\n",
       "      <td>3.760240</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>371</td>\n",
       "      <td>45</td>\n",
       "      <td>1132.516450</td>\n",
       "      <td>345.238989</td>\n",
       "      <td>44.582321</td>\n",
       "      <td>250.625000</td>\n",
       "      <td>243.250000</td>\n",
       "      <td>33.029822</td>\n",
       "      <td>2190.687500</td>\n",
       "      <td>868.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>7076</td>\n",
       "      <td>1710</td>\n",
       "      <td>82.045570</td>\n",
       "      <td>2168.857164</td>\n",
       "      <td>393.292016</td>\n",
       "      <td>19.956490</td>\n",
       "      <td>42</td>\n",
       "      <td>53</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>376</td>\n",
       "      <td>45</td>\n",
       "      <td>406.163972</td>\n",
       "      <td>277.165744</td>\n",
       "      <td>41.661910</td>\n",
       "      <td>158.812500</td>\n",
       "      <td>209.312500</td>\n",
       "      <td>42.499318</td>\n",
       "      <td>1167.125000</td>\n",
       "      <td>749.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2638</td>\n",
       "      <td>1913</td>\n",
       "      <td>93.952614</td>\n",
       "      <td>664.073931</td>\n",
       "      <td>372.279399</td>\n",
       "      <td>15.928206</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3208</th>\n",
       "      <td>377</td>\n",
       "      <td>45</td>\n",
       "      <td>757.079564</td>\n",
       "      <td>239.739090</td>\n",
       "      <td>35.574966</td>\n",
       "      <td>319.187500</td>\n",
       "      <td>249.062500</td>\n",
       "      <td>33.547344</td>\n",
       "      <td>1632.062500</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2703</td>\n",
       "      <td>1041</td>\n",
       "      <td>90.850258</td>\n",
       "      <td>849.300358</td>\n",
       "      <td>245.768238</td>\n",
       "      <td>10.676393</td>\n",
       "      <td>52</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>387</td>\n",
       "      <td>45</td>\n",
       "      <td>1089.436663</td>\n",
       "      <td>144.844978</td>\n",
       "      <td>45.498784</td>\n",
       "      <td>188.625000</td>\n",
       "      <td>172.937500</td>\n",
       "      <td>44.561787</td>\n",
       "      <td>2326.312500</td>\n",
       "      <td>458.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>4099</td>\n",
       "      <td>1315</td>\n",
       "      <td>78.417984</td>\n",
       "      <td>1207.733704</td>\n",
       "      <td>226.834053</td>\n",
       "      <td>12.316908</td>\n",
       "      <td>52</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>388</td>\n",
       "      <td>45</td>\n",
       "      <td>1329.010118</td>\n",
       "      <td>133.950538</td>\n",
       "      <td>28.093219</td>\n",
       "      <td>329.733333</td>\n",
       "      <td>179.187500</td>\n",
       "      <td>25.341157</td>\n",
       "      <td>2457.875000</td>\n",
       "      <td>383.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>4728</td>\n",
       "      <td>613</td>\n",
       "      <td>40.353252</td>\n",
       "      <td>1576.320026</td>\n",
       "      <td>147.319089</td>\n",
       "      <td>6.428800</td>\n",
       "      <td>51</td>\n",
       "      <td>33</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>389</td>\n",
       "      <td>45</td>\n",
       "      <td>750.611840</td>\n",
       "      <td>190.677776</td>\n",
       "      <td>36.879136</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>195.875000</td>\n",
       "      <td>32.018771</td>\n",
       "      <td>1495.812500</td>\n",
       "      <td>522.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>4341</td>\n",
       "      <td>1324</td>\n",
       "      <td>93.648216</td>\n",
       "      <td>1141.822376</td>\n",
       "      <td>307.452758</td>\n",
       "      <td>13.212144</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>391</td>\n",
       "      <td>45</td>\n",
       "      <td>851.340567</td>\n",
       "      <td>124.705868</td>\n",
       "      <td>45.675248</td>\n",
       "      <td>387.375000</td>\n",
       "      <td>176.750000</td>\n",
       "      <td>39.424129</td>\n",
       "      <td>1575.500000</td>\n",
       "      <td>355.312500</td>\n",
       "      <td>...</td>\n",
       "      <td>2962</td>\n",
       "      <td>726</td>\n",
       "      <td>55.378109</td>\n",
       "      <td>800.033037</td>\n",
       "      <td>150.947750</td>\n",
       "      <td>10.716890</td>\n",
       "      <td>55</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>393</td>\n",
       "      <td>45</td>\n",
       "      <td>932.650658</td>\n",
       "      <td>137.576908</td>\n",
       "      <td>31.455444</td>\n",
       "      <td>239.437500</td>\n",
       "      <td>181.500000</td>\n",
       "      <td>29.680739</td>\n",
       "      <td>2014.125000</td>\n",
       "      <td>413.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>4139</td>\n",
       "      <td>670</td>\n",
       "      <td>49.335651</td>\n",
       "      <td>1154.419934</td>\n",
       "      <td>155.345447</td>\n",
       "      <td>8.677915</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>394</td>\n",
       "      <td>45</td>\n",
       "      <td>415.041823</td>\n",
       "      <td>223.060033</td>\n",
       "      <td>36.359041</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>209.187500</td>\n",
       "      <td>30.978387</td>\n",
       "      <td>971.250000</td>\n",
       "      <td>591.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>1928</td>\n",
       "      <td>799</td>\n",
       "      <td>49.997738</td>\n",
       "      <td>484.603006</td>\n",
       "      <td>221.482240</td>\n",
       "      <td>10.176641</td>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>404</td>\n",
       "      <td>45</td>\n",
       "      <td>170.460918</td>\n",
       "      <td>261.406851</td>\n",
       "      <td>22.683192</td>\n",
       "      <td>148.562500</td>\n",
       "      <td>142.125000</td>\n",
       "      <td>14.201353</td>\n",
       "      <td>670.437500</td>\n",
       "      <td>674.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>1198</td>\n",
       "      <td>950</td>\n",
       "      <td>98.260040</td>\n",
       "      <td>271.541283</td>\n",
       "      <td>259.057080</td>\n",
       "      <td>18.424088</td>\n",
       "      <td>59</td>\n",
       "      <td>112</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>409</td>\n",
       "      <td>45</td>\n",
       "      <td>1274.508972</td>\n",
       "      <td>93.515808</td>\n",
       "      <td>44.724520</td>\n",
       "      <td>398.312500</td>\n",
       "      <td>112.250000</td>\n",
       "      <td>36.992687</td>\n",
       "      <td>2202.437500</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3660</td>\n",
       "      <td>924</td>\n",
       "      <td>58.009537</td>\n",
       "      <td>1190.400676</td>\n",
       "      <td>138.908449</td>\n",
       "      <td>13.124599</td>\n",
       "      <td>60</td>\n",
       "      <td>53</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>412</td>\n",
       "      <td>45</td>\n",
       "      <td>1143.870798</td>\n",
       "      <td>285.925321</td>\n",
       "      <td>32.577731</td>\n",
       "      <td>226.562500</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>21.403334</td>\n",
       "      <td>2386.437500</td>\n",
       "      <td>642.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>4398</td>\n",
       "      <td>1343</td>\n",
       "      <td>56.990532</td>\n",
       "      <td>1421.602720</td>\n",
       "      <td>284.560707</td>\n",
       "      <td>5.593909</td>\n",
       "      <td>63</td>\n",
       "      <td>97</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3417</th>\n",
       "      <td>413</td>\n",
       "      <td>45</td>\n",
       "      <td>0.243250</td>\n",
       "      <td>1208.918086</td>\n",
       "      <td>33.243953</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>288.812500</td>\n",
       "      <td>28.597937</td>\n",
       "      <td>8.687500</td>\n",
       "      <td>1984.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>139</td>\n",
       "      <td>2550</td>\n",
       "      <td>73.016998</td>\n",
       "      <td>6.600243</td>\n",
       "      <td>747.768783</td>\n",
       "      <td>10.841281</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>433</td>\n",
       "      <td>45</td>\n",
       "      <td>562.554638</td>\n",
       "      <td>220.089855</td>\n",
       "      <td>37.764686</td>\n",
       "      <td>128.823529</td>\n",
       "      <td>100.588235</td>\n",
       "      <td>22.597800</td>\n",
       "      <td>1371.058824</td>\n",
       "      <td>596.352941</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>935</td>\n",
       "      <td>84.657867</td>\n",
       "      <td>951.873395</td>\n",
       "      <td>235.670440</td>\n",
       "      <td>16.408542</td>\n",
       "      <td>78</td>\n",
       "      <td>123</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  PAINTING  mean_fixations_small  mean_saccades_small  \\\n",
       "17      1        45          16163.011484             7.248903   \n",
       "36      2        45           2269.519269           118.094715   \n",
       "721   100        45           1205.692659           112.501913   \n",
       "3170  371        45           1132.516450           345.238989   \n",
       "3189  376        45            406.163972           277.165744   \n",
       "3208  377        45            757.079564           239.739090   \n",
       "3227  387        45           1089.436663           144.844978   \n",
       "3246  388        45           1329.010118           133.950538   \n",
       "3265  389        45            750.611840           190.677776   \n",
       "3284  391        45            851.340567           124.705868   \n",
       "3303  393        45            932.650658           137.576908   \n",
       "3322  394        45            415.041823           223.060033   \n",
       "3341  404        45            170.460918           261.406851   \n",
       "3360  409        45           1274.508972            93.515808   \n",
       "3398  412        45           1143.870798           285.925321   \n",
       "3417  413        45              0.243250          1208.918086   \n",
       "3436  433        45            562.554638           220.089855   \n",
       "\n",
       "      mean_diametro_small  min_fixations_small  min_saccades_small  \\\n",
       "17               1.669092         15893.937500          332.600000   \n",
       "36              11.386530          1671.125000          242.642857   \n",
       "721             53.129677           371.750000          132.000000   \n",
       "3170            44.582321           250.625000          243.250000   \n",
       "3189            41.661910           158.812500          209.312500   \n",
       "3208            35.574966           319.187500          249.062500   \n",
       "3227            45.498784           188.625000          172.937500   \n",
       "3246            28.093219           329.733333          179.187500   \n",
       "3265            36.879136           199.000000          195.875000   \n",
       "3284            45.675248           387.375000          176.750000   \n",
       "3303            31.455444           239.437500          181.500000   \n",
       "3322            36.359041           190.000000          209.187500   \n",
       "3341            22.683192           148.562500          142.125000   \n",
       "3360            44.724520           398.312500          112.250000   \n",
       "3398            32.577731           226.562500          111.750000   \n",
       "3417            33.243953           139.000000          288.812500   \n",
       "3436            37.764686           128.823529          100.588235   \n",
       "\n",
       "      min_diametro_small  max_fixations_small  max_saccades_small  ...  \\\n",
       "17             23.893966         17257.235294          100.117647  ...   \n",
       "36             26.685896          3444.000000          464.562500  ...   \n",
       "721            49.204310          2047.812500          321.187500  ...   \n",
       "3170           33.029822          2190.687500          868.250000  ...   \n",
       "3189           42.499318          1167.125000          749.500000  ...   \n",
       "3208           33.547344          1632.062500          590.000000  ...   \n",
       "3227           44.561787          2326.312500          458.625000  ...   \n",
       "3246           25.341157          2457.875000          383.875000  ...   \n",
       "3265           32.018771          1495.812500          522.062500  ...   \n",
       "3284           39.424129          1575.500000          355.312500  ...   \n",
       "3303           29.680739          2014.125000          413.250000  ...   \n",
       "3322           30.978387           971.250000          591.125000  ...   \n",
       "3341           14.201353           670.437500          674.187500  ...   \n",
       "3360           36.992687          2202.437500          296.000000  ...   \n",
       "3398           21.403334          2386.437500          642.500000  ...   \n",
       "3417           28.597937             8.687500         1984.625000  ...   \n",
       "3436           22.597800          1371.058824          596.352941  ...   \n",
       "\n",
       "      max_fixations_full  max_saccades_full  max_diametro_full  \\\n",
       "17                 26047                379          48.796249   \n",
       "36                  7972               1146          45.317871   \n",
       "721                 5536                537          59.457198   \n",
       "3170                7076               1710          82.045570   \n",
       "3189                2638               1913          93.952614   \n",
       "3208                2703               1041          90.850258   \n",
       "3227                4099               1315          78.417984   \n",
       "3246                4728                613          40.353252   \n",
       "3265                4341               1324          93.648216   \n",
       "3284                2962                726          55.378109   \n",
       "3303                4139                670          49.335651   \n",
       "3322                1928                799          49.997738   \n",
       "3341                1198                950          98.260040   \n",
       "3360                3660                924          58.009537   \n",
       "3398                4398               1343          56.990532   \n",
       "3417                 139               2550          73.016998   \n",
       "3436                3272                935          84.657867   \n",
       "\n",
       "      std_fixations_full  std_saccades_full  std_diametro_full  \\\n",
       "17           7960.700631          52.836893           7.288566   \n",
       "36           2647.314151         212.889846          17.403155   \n",
       "721          1558.796177         123.704900           3.760240   \n",
       "3170         2168.857164         393.292016          19.956490   \n",
       "3189          664.073931         372.279399          15.928206   \n",
       "3208          849.300358         245.768238          10.676393   \n",
       "3227         1207.733704         226.834053          12.316908   \n",
       "3246         1576.320026         147.319089           6.428800   \n",
       "3265         1141.822376         307.452758          13.212144   \n",
       "3284          800.033037         150.947750          10.716890   \n",
       "3303         1154.419934         155.345447           8.677915   \n",
       "3322          484.603006         221.482240          10.176641   \n",
       "3341          271.541283         259.057080          18.424088   \n",
       "3360         1190.400676         138.908449          13.124599   \n",
       "3398         1421.602720         284.560707           5.593909   \n",
       "3417            6.600243         747.768783          10.841281   \n",
       "3436          951.873395         235.670440          16.408542   \n",
       "\n",
       "      num_diff_nonzero_fixations_full  num_diff_nonzero_saccades_full  voto  \\\n",
       "17                                  5                               5     0   \n",
       "36                                 45                              29     0   \n",
       "721                                74                              76     5   \n",
       "3170                               42                              53    27   \n",
       "3189                               50                              44     2   \n",
       "3208                               52                              40    42   \n",
       "3227                               52                              34    25   \n",
       "3246                               51                              33    50   \n",
       "3265                               55                              38    31   \n",
       "3284                               55                              30    37   \n",
       "3303                               52                              41    16   \n",
       "3322                               60                              48    20   \n",
       "3341                               59                             112    21   \n",
       "3360                               60                              53    22   \n",
       "3398                               63                              97    23   \n",
       "3417                                1                              62    29   \n",
       "3436                               78                             123    47   \n",
       "\n",
       "      Binary  \n",
       "17         0  \n",
       "36         0  \n",
       "721        0  \n",
       "3170       1  \n",
       "3189       0  \n",
       "3208       1  \n",
       "3227       0  \n",
       "3246       1  \n",
       "3265       1  \n",
       "3284       1  \n",
       "3303       0  \n",
       "3322       0  \n",
       "3341       0  \n",
       "3360       0  \n",
       "3398       0  \n",
       "3417       1  \n",
       "3436       1  \n",
       "\n",
       "[17 rows x 46 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['PAINTING']==45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Extracted_Features_NO-OUTLIERS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "PAINTING: 21\n",
      "Total Samples: 24\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.67\n",
      "SVM: 0.42\n",
      "Random Forest: 0.42\n",
      "Decision Tree: 0.75\n",
      "\n",
      "\n",
      "PAINTING: 9\n",
      "Total Samples: 14\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.57\n",
      "SVM: 0.57\n",
      "Random Forest: 0.71\n",
      "Decision Tree: 0.57\n",
      "\n",
      "\n",
      "PAINTING: 6\n",
      "Total Samples: 40\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.50\n",
      "SVM: 0.45\n",
      "Random Forest: 0.60\n",
      "Decision Tree: 0.70\n",
      "\n",
      "\n",
      "PAINTING: 1\n",
      "Total Samples: 52\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.62\n",
      "SVM: 0.69\n",
      "Random Forest: 0.65\n",
      "Decision Tree: 0.54\n",
      "\n",
      "\n",
      "PAINTING: 22\n",
      "Total Samples: 42\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.57\n",
      "SVM: 0.48\n",
      "Random Forest: 0.57\n",
      "Decision Tree: 0.67\n",
      "\n",
      "\n",
      "PAINTING: 16\n",
      "Total Samples: 28\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.57\n",
      "SVM: 0.36\n",
      "Random Forest: 0.64\n",
      "Decision Tree: 0.43\n",
      "\n",
      "\n",
      "PAINTING: 10\n",
      "Total Samples: 16\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.25\n",
      "SVM: 0.38\n",
      "Random Forest: 0.38\n",
      "Decision Tree: 0.62\n",
      "\n",
      "\n",
      "PAINTING: 18\n",
      "Total Samples: 42\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.48\n",
      "SVM: 0.48\n",
      "Random Forest: 0.62\n",
      "Decision Tree: 0.52\n",
      "\n",
      "\n",
      "PAINTING: 14\n",
      "Total Samples: 28\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.57\n",
      "SVM: 0.36\n",
      "Random Forest: 0.43\n",
      "Decision Tree: 0.43\n",
      "\n",
      "\n",
      "PAINTING: 7\n",
      "Total Samples: 46\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.30\n",
      "SVM: 0.57\n",
      "Random Forest: 0.35\n",
      "Decision Tree: 0.43\n",
      "\n",
      "\n",
      "PAINTING: 13\n",
      "Total Samples: 32\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.38\n",
      "SVM: 0.56\n",
      "Random Forest: 0.44\n",
      "Decision Tree: 0.56\n",
      "\n",
      "\n",
      "PAINTING: 47\n",
      "Total Samples: 36\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.39\n",
      "SVM: 0.44\n",
      "Random Forest: 0.50\n",
      "Decision Tree: 0.56\n",
      "\n",
      "\n",
      "PAINTING: 20\n",
      "Total Samples: 40\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.40\n",
      "SVM: 0.55\n",
      "Random Forest: 0.45\n",
      "Decision Tree: 0.55\n",
      "\n",
      "\n",
      "PAINTING: 2\n",
      "Total Samples: 42\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.48\n",
      "SVM: 0.48\n",
      "Random Forest: 0.52\n",
      "Decision Tree: 0.38\n",
      "\n",
      "\n",
      "PAINTING: 4\n",
      "Total Samples: 28\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.50\n",
      "SVM: 0.36\n",
      "Random Forest: 0.43\n",
      "Decision Tree: 0.43\n",
      "\n",
      "\n",
      "PAINTING: 23\n",
      "Total Samples: 38\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.47\n",
      "SVM: 0.47\n",
      "Random Forest: 0.47\n",
      "Decision Tree: 0.47\n",
      "\n",
      "\n",
      "PAINTING: 15\n",
      "Total Samples: 42\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.43\n",
      "SVM: 0.43\n",
      "Random Forest: 0.43\n",
      "Decision Tree: 0.33\n",
      "\n",
      "\n",
      "PAINTING: 11\n",
      "Total Samples: 34\n",
      "Percent 0: 50.00%\n",
      "Percent 1: 50.00%\n",
      "Logistic Regression: 0.35\n",
      "SVM: 0.41\n",
      "Random Forest: 0.41\n",
      "Decision Tree: 0.29\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[features]\n",
    "y = data['Binary']\n",
    "\n",
    "# Dizionario per memorizzare i risultati\n",
    "results = {}\n",
    "\n",
    "for painting in data['PAINTING'].unique():\n",
    "    try:\n",
    "        subset = data[data['PAINTING'] == painting]\n",
    "\n",
    "        # Bilanciamento delle classi\n",
    "        min_class_size = subset['Binary'].value_counts().min()\n",
    "        balanced_subset = subset.groupby('Binary').sample(n=min_class_size, random_state=42)\n",
    "\n",
    "        subset = balanced_subset\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(subset[features], subset['Binary'], test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Calcolo delle percentuali di 0 e 1 nella colonna 'Binary'\n",
    "        count_binary = subset['Binary'].value_counts(normalize=True) * 100\n",
    "        percent_0 = count_binary.get(0, 0)\n",
    "        percent_1 = count_binary.get(1, 0)\n",
    "        \n",
    "        # Creazione dei modelli\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "            'SVM': SVC(),\n",
    "            'Random Forest': RandomForestClassifier(),\n",
    "            'Decision Tree': DecisionTreeClassifier()\n",
    "        }\n",
    "        \n",
    "        # Dizionario per memorizzare l'accuratezza per ogni modello\n",
    "        accuracies = {}\n",
    "        \n",
    "        # Training e test per ogni modello\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracies[model_name] = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Memorizzazione dei risultati\n",
    "        results[painting] = {\n",
    "            'Accuracy': accuracies,\n",
    "            'Percent 0': percent_0,\n",
    "            'Percent 1': percent_1,\n",
    "            'Total Samples': len(subset)\n",
    "        }\n",
    "    except:\n",
    "        print(painting)\n",
    "\n",
    "# Step 3: Ordinamento dei risultati\n",
    "sorted_results = {k: v for k, v in sorted(results.items(), key=lambda item: max(item[1]['Accuracy'].values()), reverse=True)}\n",
    "\n",
    "# Stampa dei risultati in formato report\n",
    "for painting, info in sorted_results.items():\n",
    "    print(f\"PAINTING: {painting}\")\n",
    "    print(f\"Total Samples: {info['Total Samples']}\")\n",
    "    print(f\"Percent 0: {info['Percent 0']:.2f}%\")\n",
    "    print(f\"Percent 1: {info['Percent 1']:.2f}%\")\n",
    "    for model, accuracy in info['Accuracy'].items():\n",
    "        print(f\"{model}: {accuracy:.2f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, 0.75],\n",
       " [9, 0.7142857142857143],\n",
       " [6, 0.7],\n",
       " [1, 0.6923076923076923],\n",
       " [22, 0.6666666666666666],\n",
       " [16, 0.6428571428571429],\n",
       " [10, 0.625],\n",
       " [18, 0.6190476190476191],\n",
       " [14, 0.5714285714285714],\n",
       " [7, 0.5652173913043478],\n",
       " [13, 0.5625],\n",
       " [47, 0.5555555555555556],\n",
       " [20, 0.55],\n",
       " [2, 0.5238095238095238],\n",
       " [4, 0.5],\n",
       " [23, 0.47368421052631576],\n",
       " [15, 0.42857142857142855],\n",
       " [11, 0.4117647058823529]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista per contenere le coppie [id, max_accuracy]\n",
    "max_accuracy_list = []\n",
    "\n",
    "# Estrazione dell'accuracy più alta per ogni ID\n",
    "for id, values in sorted_results.items():\n",
    "    max_accuracy = max(values['Accuracy'].values())  # Trova il massimo tra le accuracies\n",
    "    max_accuracy_list.append([id, max_accuracy])  # Aggiunge la coppia [id, max_accuracy] alla lista\n",
    "\n",
    "max_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 25.574074074074073), (2, 23.053571428571427), (4, 30.604166666666668), (6, 26.680851063829788), (7, 25.32), (9, 32.56521739130435), (10, 34.03225806451613), (11, 28.942307692307693), (13, 20.904761904761905), (14, 19.227272727272727), (15, 27.444444444444443), (16, 20.83783783783784), (18, 26.0), (20, 26.916666666666668), (21, 28.833333333333332), (22, 27.470588235294116), (23, 23.205128205128204), (45, 27.666666666666668), (47, 21.137254901960784)]\n"
     ]
    }
   ],
   "source": [
    "voto_medio_per_painting = data.groupby('PAINTING')['voto'].mean().reset_index()\n",
    "\n",
    "# Rinominiamo le colonne per chiarezza\n",
    "voto_medio_per_painting.columns = ['PAINTING', 'Voto Medio']\n",
    "\n",
    "# Convertiamo il DataFrame in una lista di tuple\n",
    "lista_voti_medio = list(voto_medio_per_painting.itertuples(index=False, name=None))\n",
    "\n",
    "# Mostra la lista\n",
    "print(lista_voti_medio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertiamo le liste in DataFrame\n",
    "df_accuracy = pd.DataFrame(max_accuracy_list, columns=['id', 'accuracy'])\n",
    "df_voti = pd.DataFrame(lista_voti_medio, columns=['id', 'voto'])\n",
    "\n",
    "# Uniamo i DataFrame sui valori di 'id'\n",
    "df_merged = pd.merge(df_accuracy, df_voti, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creazione dello scatter plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df_merged['voto'], df_merged['accuracy'], color='blue', alpha=0.5)\n",
    "\n",
    "# Aggiungiamo titoli agli assi e al grafico\n",
    "plt.title('Scatter Plot di Accuracy vs Voto')\n",
    "plt.xlabel('Voto Medio')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "voti = [voto for _, voto in lista_voti_medio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creiamo un histogramma dei voti\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(voti, bins=10, color='skyblue', alpha=0.7, rwidth=0.85)\n",
    "plt.title('Distribuzione dei Voti')\n",
    "plt.xlabel('Voti')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creiamo un histogramma dei voti\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data['voto'], bins=50, color='skyblue', alpha=0.7, rwidth=0.85)\n",
    "plt.title('Distribuzione dei Voti')\n",
    "plt.xlabel('Voti')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
